{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport locale\nfrom locale import atof\n\nlocale.setlocale(locale.LC_NUMERIC, '')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:55.318664Z","iopub.execute_input":"2021-12-15T22:19:55.319227Z","iopub.status.idle":"2021-12-15T22:19:55.325904Z","shell.execute_reply.started":"2021-12-15T22:19:55.319190Z","shell.execute_reply":"2021-12-15T22:19:55.325343Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### First we'll be importing the dataset as a pandas file. Next we'll use some basic functions that describe the data, in order to better understand it.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(r'../input/dataset1csv/dataset1.csv', thousands=',')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:55.336615Z","iopub.execute_input":"2021-12-15T22:19:55.337262Z","iopub.status.idle":"2021-12-15T22:19:55.368699Z","shell.execute_reply.started":"2021-12-15T22:19:55.337229Z","shell.execute_reply":"2021-12-15T22:19:55.367877Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-15T22:19:55.370141Z","iopub.execute_input":"2021-12-15T22:19:55.370481Z","iopub.status.idle":"2021-12-15T22:19:55.384087Z","shell.execute_reply.started":"2021-12-15T22:19:55.370444Z","shell.execute_reply":"2021-12-15T22:19:55.383317Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"df.describe() ","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:55.385366Z","iopub.execute_input":"2021-12-15T22:19:55.385816Z","iopub.status.idle":"2021-12-15T22:19:55.425453Z","shell.execute_reply.started":"2021-12-15T22:19:55.385738Z","shell.execute_reply":"2021-12-15T22:19:55.424832Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"df[df['State'].str.contains(\"Columbia\")]","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:55.426926Z","iopub.execute_input":"2021-12-15T22:19:55.427233Z","iopub.status.idle":"2021-12-15T22:19:55.441806Z","shell.execute_reply.started":"2021-12-15T22:19:55.427207Z","shell.execute_reply":"2021-12-15T22:19:55.440991Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:55.443473Z","iopub.execute_input":"2021-12-15T22:19:55.444297Z","iopub.status.idle":"2021-12-15T22:19:55.478831Z","shell.execute_reply.started":"2021-12-15T22:19:55.444251Z","shell.execute_reply":"2021-12-15T22:19:55.478015Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"#### The immediate problem I noticed was that there were 52 rows entered, while they are in fact 50 states. With the help of the .head method, we can see that they included the US as a whole on the first entry, which we can remove. This still leaves us with one extra state. After some searching it seems that people often confuse D.C or Distric of Columbia as a state. Sure, enough I found the a column with the state of D.C, which is incorrect if we were to do a state analysis. \n\n### Now onto the first problem, which is pre-processing the 'State' column\n\n#### As seen previosly the two rows we need to delete if we were to do a analysis based by states would be entries 0 and 9, which represt the US and DC respectively. We'll create another dataset for that as we don't want to alter the original one.","metadata":{}},{"cell_type":"code","source":"state_df = df.drop([0,9])\nstate_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:55.480098Z","iopub.execute_input":"2021-12-15T22:19:55.480469Z","iopub.status.idle":"2021-12-15T22:19:55.494830Z","shell.execute_reply.started":"2021-12-15T22:19:55.480429Z","shell.execute_reply":"2021-12-15T22:19:55.494004Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"#### Above we can see that the state_df dataframe now only contains 50 entries(i.e all the states).\n\n### Next up we need to find all rows where the vote for highest office(President) contain a NaN value and return the VEP turnout rate for them.\n","metadata":{}},{"cell_type":"code","source":"na_df = df[df['Vote for Highest Office (President)'].isna()]\nna_df","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-15T22:19:55.496122Z","iopub.execute_input":"2021-12-15T22:19:55.496573Z","iopub.status.idle":"2021-12-15T22:19:55.521982Z","shell.execute_reply.started":"2021-12-15T22:19:55.496528Z","shell.execute_reply":"2021-12-15T22:19:55.521075Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"#### Above is all the entries with a NaN value for the 'vote for highest office (president)' column. Next we'll just print out the state name and VEP turnout rate","metadata":{}},{"cell_type":"code","source":"na_df[['State','VEP Turnout Rate']]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-15T22:19:55.523211Z","iopub.execute_input":"2021-12-15T22:19:55.523542Z","iopub.status.idle":"2021-12-15T22:19:55.542353Z","shell.execute_reply.started":"2021-12-15T22:19:55.523514Z","shell.execute_reply":"2021-12-15T22:19:55.541737Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"### ","metadata":{}},{"cell_type":"markdown","source":"### Now onto the predictive model. I'll be using a simple linear regression model as it works well with the data provided. I'll write all the parts of the model out instead of making one class or function.\n\n#### We need to arrays, one for the inputs and the other for the  targets","metadata":{}},{"cell_type":"code","source":"inputs = df[['Total Ineligible Felon', 'Parole', 'Prison', '% Non-citizen', 'Voting-Age Population (VAP)']]\ntargets = df['Total Ballots Counted (Estimate)']\ninputs['% Non-citizen'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\ninputs = inputs.to_numpy()\ntargets = targets.to_numpy()\ninputs = inputs.astype('float32')\ntargets = targets.astype('float32')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:55.545367Z","iopub.execute_input":"2021-12-15T22:19:55.545783Z","iopub.status.idle":"2021-12-15T22:19:55.553339Z","shell.execute_reply.started":"2021-12-15T22:19:55.545751Z","shell.execute_reply":"2021-12-15T22:19:55.552371Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"#### I had forgetten but at this point I realized that a lot of entries were in string form with commas representing thousands. In order to convert them I made use of locale and changed the way I read the csv file at the beginning. As well as removing the % from the non-citizen column. This would normally belong in the data cleanup right at the beginning.","metadata":{}},{"cell_type":"code","source":"inputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)\ntargets.resize_(52,1);","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:55.567369Z","iopub.execute_input":"2021-12-15T22:19:55.567674Z","iopub.status.idle":"2021-12-15T22:19:55.572272Z","shell.execute_reply.started":"2021-12-15T22:19:55.567645Z","shell.execute_reply":"2021-12-15T22:19:55.571384Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"train_ds = TensorDataset(inputs, targets)\nval_ds = TensorDataset(inputs, targets)\nbatch_size = 16\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size*2)\nmodel = nn.Linear(5, 1)\npreds = model(inputs)\nloss_fn = F.l1_loss\nparams = [inputs,targets]\nopt = torch.optim.RMSprop(params, lr=1e-10, alpha=0.99, eps=1e-10, weight_decay=0, momentum=0, centered=False);","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:55.573951Z","iopub.execute_input":"2021-12-15T22:19:55.574174Z","iopub.status.idle":"2021-12-15T22:19:55.584093Z","shell.execute_reply.started":"2021-12-15T22:19:55.574148Z","shell.execute_reply":"2021-12-15T22:19:55.583326Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def fit(num_epochs, model, loss_fn, opt, train_dl):\n    for epoch in range(num_epochs):\n        for xb,yb in train_dl:\n            pred = model(xb)\n            loss = loss_fn(pred, yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        if (epoch+1) % 100 == 0:\n            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:55.590169Z","iopub.execute_input":"2021-12-15T22:19:55.590455Z","iopub.status.idle":"2021-12-15T22:19:55.597264Z","shell.execute_reply.started":"2021-12-15T22:19:55.590400Z","shell.execute_reply":"2021-12-15T22:19:55.596389Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"fit(2000, model, loss_fn, opt, train_dl)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-15T22:19:55.598716Z","iopub.execute_input":"2021-12-15T22:19:55.599270Z","iopub.status.idle":"2021-12-15T22:19:58.311576Z","shell.execute_reply.started":"2021-12-15T22:19:55.599233Z","shell.execute_reply":"2021-12-15T22:19:58.310493Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"preds = model(inputs)\npreds","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-15T22:19:58.312806Z","iopub.execute_input":"2021-12-15T22:19:58.313002Z","iopub.status.idle":"2021-12-15T22:19:58.321220Z","shell.execute_reply.started":"2021-12-15T22:19:58.312978Z","shell.execute_reply":"2021-12-15T22:19:58.320151Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"targets","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-15T22:19:58.322495Z","iopub.execute_input":"2021-12-15T22:19:58.322947Z","iopub.status.idle":"2021-12-15T22:19:58.333560Z","shell.execute_reply.started":"2021-12-15T22:19:58.322919Z","shell.execute_reply":"2021-12-15T22:19:58.332611Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### Unfortunatly I have made quite a few mistakes with how I went about making a model. I would scrap it and build a better feedforward model but I'm short on time so I'll focus on the last part first.\n\n#### We'll start off by looking at the top 10 states with the highest VEP turnout","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:58.335548Z","iopub.execute_input":"2021-12-15T22:19:58.336365Z","iopub.status.idle":"2021-12-15T22:19:58.343394Z","shell.execute_reply.started":"2021-12-15T22:19:58.336318Z","shell.execute_reply":"2021-12-15T22:19:58.342482Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"turn_out = df.sort_values(by = ['VEP Turnout Rate'], ascending = False)\nturn_out[['State','VEP Turnout Rate']].head(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:58.344303Z","iopub.execute_input":"2021-12-15T22:19:58.344938Z","iopub.status.idle":"2021-12-15T22:19:58.362695Z","shell.execute_reply.started":"2021-12-15T22:19:58.344896Z","shell.execute_reply":"2021-12-15T22:19:58.361944Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"turn_out[['State','VEP Turnout Rate']].tail(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:19:58.365114Z","iopub.execute_input":"2021-12-15T22:19:58.366700Z","iopub.status.idle":"2021-12-15T22:19:58.377113Z","shell.execute_reply.started":"2021-12-15T22:19:58.366665Z","shell.execute_reply":"2021-12-15T22:19:58.376269Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"top10 = Image.open('../input/election-images/top10.png')\nbottom10 = Image.open('../input/election-images/bottom10.png')\ntop10","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:20:42.089969Z","iopub.execute_input":"2021-12-15T22:20:42.090477Z","iopub.status.idle":"2021-12-15T22:20:43.966638Z","shell.execute_reply.started":"2021-12-15T22:20:42.090444Z","shell.execute_reply":"2021-12-15T22:20:43.965591Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"bottom10","metadata":{"execution":{"iopub.status.busy":"2021-12-15T22:20:57.114278Z","iopub.execute_input":"2021-12-15T22:20:57.114972Z","iopub.status.idle":"2021-12-15T22:20:58.933372Z","shell.execute_reply.started":"2021-12-15T22:20:57.114930Z","shell.execute_reply":"2021-12-15T22:20:58.932134Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}